{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR: End-to-End Object Detection with Transformers\n",
    "\n",
    "- Paper: [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)\n",
    "- Official video: [DETR - End to end object detection with transformers (ECCV2020)](https://youtu.be/utxbUlo9CyY?si=cIHkqMDQj7rMrTCk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchinfo import summary\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "import dataloader\n",
    "import config\n",
    "import utils\n",
    "import engine\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = config.DATASET_DIR\n",
    "PARENT_DIR = config.PARENT_DIR\n",
    "\n",
    "IMAGE_HEIGHT = config.IMAGE_HEIGHT\n",
    "IMAGE_WIDTH = config.IMAGE_WIDTH\n",
    "\n",
    "PIN_MEMORY = config.PIN_MEMORY\n",
    "LEARNING_RATE = config.LEARNING_RATE\n",
    "BATCH_SIZE = config.BATCH_SIZE\n",
    "NUM_EPOCHS = config.NUM_EPOCHS\n",
    "NUM_WORKERS = config.NUM_WORKERS\n",
    "\n",
    "SEED = config.SEED\n",
    "PRINT_MODEL_SUMMARY = config.PRINT_MODEL_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path to data folder\n",
    "data_path = Path(DATASET_DIR)\n",
    "image_path = data_path / \"images\"\n",
    "label_path = data_path / \"labels\"\n",
    "class_names_file = data_path / \"class_names.txt\"\n",
    "\n",
    "train_csv_file = data_path / \"train.csv\"\n",
    "test_csv_file = data_path / \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read class names txt file\n",
    "with open(class_names_file, \"r\") as f:\n",
    "    classes = [class_name.strip() for class_name in f.readlines()]\n",
    "\n",
    "# Create class to index dictionary\n",
    "class_to_idx = {int(class_name.split(\":\")[0]): class_name.split(\":\")[1].strip().strip(\"'\") for i, class_name in enumerate(classes)}\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(f\"Number of classes: {len(classes)}\")\n",
    "print(f\"Class to index: {class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv_file, header=None, names=[\"img\", \"label\"])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations transforms\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomCrop(224, 224, p=0.5),\n",
    "    A.ColorJitter(p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['labels']))\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['labels'])\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader = dataloader.get_loaders(dataset_dir=train_csv_file,\n",
    "                                image_dir=image_path,\n",
    "                                label_dir=label_path,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                transform=train_transform,\n",
    "                                set_type=\"train\",\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                pin_memory=True)\n",
    "\n",
    "test_dataloader = dataloader.get_loaders(dataset_dir=test_csv_file,\n",
    "                                image_dir=image_path,\n",
    "                                label_dir=label_path,\n",
    "                                batch_size=1,\n",
    "                                transform=test_transform,\n",
    "                                set_type=\"test\",\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize samples from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 subplot\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "colors = utils.get_color_map()\n",
    "\n",
    "# Iterate over the first 9 images from the dataloader\n",
    "for i, (image, target) in enumerate(itertools.islice(val_dataloader, 9)):\n",
    "    # Convert the image tensor to a numpy array and denormalize it\n",
    "    image = image[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    image = (image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
    "    image = image.astype(np.uint8)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    bboxes = target['boxes'].numpy()[0]\n",
    "    labels = target['labels'].numpy()[0]\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Draw bboxes\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        x *= width\n",
    "        y *= height\n",
    "        w *= width\n",
    "        h *= height\n",
    "\n",
    "        left = int(x - w / 2)\n",
    "        top = int(y - h / 2)\n",
    "\n",
    "        cv2.rectangle(image, (int(left), int(top)), (int(left + w), int(top + h)), colors[int(label)], 2)\n",
    "\n",
    "        text = f'{class_to_idx[int(label)]}'\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        cv2.rectangle(image, (left, top - text_size[1] - 5), (left + text_size[0] + 5, top), colors[int(label)], -1)\n",
    "        cv2.putText(image, text, (left + 2, top - 5), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "    # Get current subplot\n",
    "    ax = axs[i // 3, i % 3]\n",
    "\n",
    "    # Show the image, converted to RGB for matplotlib\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "<img src=\"notebook_images/detr_overall.png\" width=\"900\">\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Matching\n",
    "<img src=\"notebook_images/detr_output_matching.png\" width=\"900\">\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Architecture\n",
    "<img src=\"notebook_images/detr_architecture.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model.DETRdemo(num_classes=20).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "if PRINT_MODEL_SUMMARY:\n",
    "        from torchinfo import summary\n",
    "\n",
    "        summary(model=net,\n",
    "                input_size=(1, 3, 224, 224), # (batch_size, channels, height, width)\n",
    "                col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "                col_width=20,\n",
    "                row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_fn import SetCriterion\n",
    "from hungarian_matcher import HungarianMatcher\n",
    "\n",
    "matcher = HungarianMatcher()\n",
    "\n",
    "weight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "NULL_CLASS_COEF = 0.5\n",
    "\n",
    "criterion = SetCriterion(NUM_CLASSES-1, matcher, weight_dict, eos_coef = NULL_CLASS_COEF, losses=losses).to(DEVICE) # eos_coef is used in the output layer to affect the output corresponding to the absence of an object.\n",
    "\n",
    "optimizer = torch.optim.Adam(params=net.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results = engine.train(model=net,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      test_dataloader=test_dataloader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=criterion,\n",
    "                      epochs=NUM_EPOCHS,\n",
    "                      device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy curves\n",
    "utils.plot_loss_curves(results=results, save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
