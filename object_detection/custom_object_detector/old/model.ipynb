{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Object Detector\n",
    "\n",
    "Reference: [Training an object detector from scratch in PyTorch](https://pyimagesearch.com/2021/11/01/training-an-object-detector-from-scratch-in-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Chihuahua', \n",
    "           'Golden_retriever', \n",
    "           'Welsh_springer_spaniel', \n",
    "           'German_shepherd', \n",
    "           'Doberman', \n",
    "           'Boxer', \n",
    "           'Siberian_husky', \n",
    "           'Pug', \n",
    "           'Pomeranian', \n",
    "           'Cardigan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base path to the input dataset and then use it to derive\n",
    "# the path to the input images and annotation CSV files\n",
    "BASE_PATH = \"dataset/standford_dogs_mini_10/yolo\"\n",
    "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
    "IMAGES_FILE_PATHs = os.path.sep.join([BASE_PATH, \"image_paths.txt\"])\n",
    "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"annotations\"])\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "# define the path to the output model, label encoder, plots output\n",
    "# directory, and testing image paths\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.pth\"])\n",
    "LE_PATH = os.path.sep.join([BASE_OUTPUT, \"le.pickle\"])\n",
    "PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the current device and based on that set the pin memory\n",
    "# flag\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "# specify ImageNet mean and standard deviation\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "# initialize our initial learning rate, number of epochs to train\n",
    "# for, and the batch size\n",
    "INIT_LR = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "# specify the loss weights\n",
    "LABELS = 1.0\n",
    "BBOX = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self,tensors, transforms=None):\n",
    "        self.tensors = tensors\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the size of the dataset\n",
    "        return self.tensors[0].size(0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # grab the image, label, and its bounding box coordinates\n",
    "        image = self.tensors[0][index]\n",
    "        label = self.tensors[1][index]\n",
    "        bbox = self.tensors[2][index]\n",
    "        # transpose the image such that its channel dimension becomes\n",
    "        # the leading one\n",
    "        image = image.permute(2, 0, 1)\n",
    "        # check to see if we have any image transformations to apply\n",
    "        # and if so, apply them\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        # return a tuple of the images, labels, and bounding\n",
    "        # box coordinates\n",
    "        return (image, label, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "    def __init__(self, basemodel, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = basemodel\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(basemodel.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, self.num_classes),\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(basemodel.fc.in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        classlogits = self.classifier(features)\n",
    "        bboxes = self.regressor(features)\n",
    "        return (bboxes, classlogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/kpatel2s/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:04<00:00, 23.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "ObjectDetector (ObjectDetector)               [1, 3, 224, 224]     [1, 4]               --                   True\n",
       "├─ResNet (backbone)                           [1, 3, 224, 224]     [1, 2048]            --                   True\n",
       "│    └─Conv2d (conv1)                         [1, 3, 224, 224]     [1, 64, 112, 112]    9,408                True\n",
       "│    └─BatchNorm2d (bn1)                      [1, 64, 112, 112]    [1, 64, 112, 112]    128                  True\n",
       "│    └─ReLU (relu)                            [1, 64, 112, 112]    [1, 64, 112, 112]    --                   --\n",
       "│    └─MaxPool2d (maxpool)                    [1, 64, 112, 112]    [1, 64, 56, 56]      --                   --\n",
       "│    └─Sequential (layer1)                    [1, 64, 56, 56]      [1, 256, 56, 56]     --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 64, 56, 56]      [1, 256, 56, 56]     75,008               True\n",
       "│    │    └─Bottleneck (1)                    [1, 256, 56, 56]     [1, 256, 56, 56]     70,400               True\n",
       "│    │    └─Bottleneck (2)                    [1, 256, 56, 56]     [1, 256, 56, 56]     70,400               True\n",
       "│    └─Sequential (layer2)                    [1, 256, 56, 56]     [1, 512, 28, 28]     --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 256, 56, 56]     [1, 512, 28, 28]     379,392              True\n",
       "│    │    └─Bottleneck (1)                    [1, 512, 28, 28]     [1, 512, 28, 28]     280,064              True\n",
       "│    │    └─Bottleneck (2)                    [1, 512, 28, 28]     [1, 512, 28, 28]     280,064              True\n",
       "│    │    └─Bottleneck (3)                    [1, 512, 28, 28]     [1, 512, 28, 28]     280,064              True\n",
       "│    └─Sequential (layer3)                    [1, 512, 28, 28]     [1, 1024, 14, 14]    --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 512, 28, 28]     [1, 1024, 14, 14]    1,512,448            True\n",
       "│    │    └─Bottleneck (1)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    1,117,184            True\n",
       "│    │    └─Bottleneck (2)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    1,117,184            True\n",
       "│    │    └─Bottleneck (3)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    1,117,184            True\n",
       "│    │    └─Bottleneck (4)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    1,117,184            True\n",
       "│    │    └─Bottleneck (5)                    [1, 1024, 14, 14]    [1, 1024, 14, 14]    1,117,184            True\n",
       "│    └─Sequential (layer4)                    [1, 1024, 14, 14]    [1, 2048, 7, 7]      --                   True\n",
       "│    │    └─Bottleneck (0)                    [1, 1024, 14, 14]    [1, 2048, 7, 7]      6,039,552            True\n",
       "│    │    └─Bottleneck (1)                    [1, 2048, 7, 7]      [1, 2048, 7, 7]      4,462,592            True\n",
       "│    │    └─Bottleneck (2)                    [1, 2048, 7, 7]      [1, 2048, 7, 7]      4,462,592            True\n",
       "│    └─AdaptiveAvgPool2d (avgpool)            [1, 2048, 7, 7]      [1, 2048, 1, 1]      --                   --\n",
       "│    └─Identity (fc)                          [1, 2048]            [1, 2048]            --                   --\n",
       "├─Sequential (classifier)                     [1, 2048]            [1, 10]              --                   True\n",
       "│    └─Linear (0)                             [1, 2048]            [1, 512]             1,049,088            True\n",
       "│    └─ReLU (1)                               [1, 512]             [1, 512]             --                   --\n",
       "│    └─Dropout (2)                            [1, 512]             [1, 512]             --                   --\n",
       "│    └─Linear (3)                             [1, 512]             [1, 512]             262,656              True\n",
       "│    └─ReLU (4)                               [1, 512]             [1, 512]             --                   --\n",
       "│    └─Dropout (5)                            [1, 512]             [1, 512]             --                   --\n",
       "│    └─Linear (6)                             [1, 512]             [1, 10]              5,130                True\n",
       "├─Sequential (regressor)                      [1, 2048]            [1, 4]               --                   True\n",
       "│    └─Linear (0)                             [1, 2048]            [1, 128]             262,272              True\n",
       "│    └─ReLU (1)                               [1, 128]             [1, 128]             --                   --\n",
       "│    └─Linear (2)                             [1, 128]             [1, 64]              8,256                True\n",
       "│    └─ReLU (3)                               [1, 64]              [1, 64]              --                   --\n",
       "│    └─Linear (4)                             [1, 64]              [1, 32]              2,080                True\n",
       "│    └─ReLU (5)                               [1, 32]              [1, 32]              --                   --\n",
       "│    └─Linear (6)                             [1, 32]              [1, 4]               132                  True\n",
       "│    └─Sigmoid (7)                            [1, 4]               [1, 4]               --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 25,097,646\n",
       "Trainable params: 25,097,646\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.09\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.83\n",
       "Params size (MB): 100.39\n",
       "Estimated Total Size (MB): 278.83\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the network\n",
    "from torchview import draw_graph\n",
    "from torchinfo import summary\n",
    "\n",
    "num_classes = 10\n",
    "IMAGE_SIZE = 224\n",
    "# Create the network\n",
    "weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "basemodel = torchvision.models.resnet50(weights=weights)\n",
    "model = ObjectDetector(basemodel, num_classes)\n",
    "\n",
    "x = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))    \n",
    "\n",
    "summary(model=model, \n",
    "        input_size=(1, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "\n",
    "# # Visualize the network\n",
    "# model_graph = draw_graph(model, x, graph_name='YOLOv3')\n",
    "# model_graph.resize_graph(scale=1.0)\n",
    "# model_graph.visual_graph.render(format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python train.py\n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "# initialize the list of data (images), class labels, target bounding\n",
    "# box coordinates, and image paths\n",
    "print(\"[INFO] loading dataset...\")\n",
    "data = []\n",
    "labels = []\n",
    "bboxes = []\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_xyxy(xywh):\n",
    "    \"\"\"\n",
    "    Convert XYWH format (x,y center point and width, height) to XYXY format (x,y top left and x,y bottom right).\n",
    "    :param xywh: [X, Y, W, H]\n",
    "    :return: [X1, Y1, X2, Y2]\n",
    "    \"\"\"\n",
    "    if np.array(xywh).ndim > 1 or len(xywh) > 4:\n",
    "        raise ValueError('xywh format: [x1, y1, width, height]')\n",
    "    xywh = [float(value) for value in xywh]\n",
    "    x1 = xywh[0] - xywh[2] / 2\n",
    "    y1 = xywh[1] - xywh[3] / 2\n",
    "    x2 = xywh[0] + xywh[2] / 2\n",
    "    y2 = xywh[1] + xywh[3] / 2\n",
    "    return np.array([int(x1), int(y1), int(x2), int(y2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "with open(IMAGES_FILE_PATHs, 'r') as f:\n",
    "    image_file_paths_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "# loop over all CSV files in the annotations directory\n",
    "for txtPath in tqdm(paths.list_files(ANNOTS_PATH, validExts=(\".txt\"))):\n",
    "\tbasename = os.path.basename(txtPath)\n",
    "\tbasename_no_ext = os.path.splitext(basename)[0]\n",
    "\n",
    "\t# get image path\n",
    "\tfor filename in image_file_paths_list:\n",
    "\t\tif basename_no_ext in filename:\n",
    "\t\t\timagePath = filename.strip()\n",
    "\t\t\tbreak\n",
    "\t\n",
    "\tif imagePath is None:\n",
    "\t\tcontinue\n",
    "\t\tprint(\"imagePath is None\")\n",
    "\n",
    "\t# load the contents of the current CSV annotations file\n",
    "\trows = open(txtPath).read().strip().split(\"\\n\")\n",
    "\t# loop over the rows\n",
    "\tfor row in rows:\n",
    "\t\t# break the row into the filename, bounding box coordinates,\n",
    "\t\t# and class label\n",
    "\t\trow = row.split(\" \")\n",
    "\t\t# convert xywh to xyxy\n",
    "\t\trow[1:] = xywh_to_xyxy(row[1:])\n",
    "\t\t(startX, startY, endX, endY, label) = row\n",
    "\t\t# derive the path to the input image, load the image (in\n",
    "\t\t# OpenCV format), and grab its dimensions\n",
    "\t\t# imagePath = os.path.sep.join([IMAGES_PATH, label,\n",
    "\t\t# \tfilename])\n",
    "\t\timage = cv2.imread(imagePath)\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\t# scale the bounding box coordinates relative to the spatial\n",
    "\t\t# dimensions of the input image\n",
    "\t\tstartX = float(startX) / w\n",
    "\t\tstartY = float(startY) / h\n",
    "\t\tendX = float(endX) / w\n",
    "\t\tendY = float(endY) / h\n",
    "\t\t# load the image and preprocess it\n",
    "\t\t# image = cv2.imread(imagePath)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\timage = cv2.resize(image, (224, 224))\n",
    "\t\t# update our list of data, class labels, bounding boxes, and\n",
    "\t\t# image paths\n",
    "\t\tdata.append(image)\n",
    "\t\tlabels.append(label)\n",
    "\t\tbboxes.append((startX, startY, endX, endY))\n",
    "\t\timagePaths.append(imagePath)\n",
    "\t\tcnt+=1\n",
    "print(\"cnt\",cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data, class labels, bounding boxes, and image paths to\n",
    "# NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "imagePaths = np.array(imagePaths)\n",
    "# perform label encoding on the labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "split = train_test_split(data, labels, bboxes, imagePaths,\n",
    "\ttest_size=0.20, random_state=42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainLabels, testLabels) = split[2:4]\n",
    "(trainBBoxes, testBBoxes) = split[4:6]\n",
    "(trainPaths, testPaths) = split[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NumPy arrays to PyTorch tensors\n",
    "(trainImages, testImages) = torch.tensor(trainImages),\\\n",
    "\ttorch.tensor(testImages)\n",
    "(trainLabels, testLabels) = torch.tensor(trainLabels),\\\n",
    "\ttorch.tensor(testLabels)\n",
    "(trainBBoxes, testBBoxes) = torch.tensor(trainBBoxes),\\\n",
    "\ttorch.tensor(testBBoxes)\n",
    "# define normalization transforms\n",
    "transforms = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=MEAN, std=STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NumPy arrays to PyTorch datasets\n",
    "trainDS = CustomTensorDataset((trainImages, trainLabels, trainBBoxes),\n",
    "\ttransforms=transforms)\n",
    "testDS = CustomTensorDataset((testImages, testLabels, testBBoxes),\n",
    "\ttransforms=transforms)\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // BATCH_SIZE\n",
    "valSteps = len(testDS) // BATCH_SIZE\n",
    "# create data loaders\n",
    "trainLoader = DataLoader(trainDS, batch_size=BATCH_SIZE,\n",
    "\tshuffle=True, num_workers=os.cpu_count(), pin_memory=PIN_MEMORY)\n",
    "testLoader = DataLoader(testDS, batch_size=BATCH_SIZE,\n",
    "\tnum_workers=os.cpu_count(), pin_memory=PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the testing image paths to disk so that we can use then\n",
    "# when evaluating/testing our object detector\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "try:\n",
    "\tf = open(TEST_PATHS, \"w\")\n",
    "except:\n",
    "\t# if the file doesn't exist, create the file using OS\n",
    "\tif not os.path.exists(TEST_PATHS):\n",
    "\t\tos.makedirs(os.path.dirname(TEST_PATHS))\n",
    "\tf = open(TEST_PATHS, \"w\")\n",
    "\n",
    "f.write(\"\\n\".join(testPaths))\n",
    "f.close()\n",
    "# load the ResNet50 network\n",
    "resnet = resnet50(pretrained=True)\n",
    "# freeze all ResNet50 layers so they will *not* be updated during the\n",
    "# training process\n",
    "for param in resnet.parameters():\n",
    "\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom object detector model and flash it to the current\n",
    "# device\n",
    "objectDetector = ObjectDetector(resnet, len(le.classes_))\n",
    "objectDetector = objectDetector.to(DEVICE)\n",
    "# define our loss functions\n",
    "classLossFunc = CrossEntropyLoss()\n",
    "bboxLossFunc = MSELoss()\n",
    "# initialize the optimizer, compile the model, and show the model\n",
    "# summary\n",
    "opt = Adam(objectDetector.parameters(), lr=INIT_LR)\n",
    "print(objectDetector)\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"total_train_loss\": [], \"total_val_loss\": [], \"train_class_acc\": [],\n",
    "\t \"val_class_acc\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tobjectDetector.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t\n",
    "    # loop over the training set\n",
    "\tfor (images, labels, bboxes) in trainLoader:\n",
    "\t\t# send the input to the device\n",
    "\t\t(images, labels, bboxes) = (images.to(DEVICE),\n",
    "\t\t\tlabels.to(DEVICE), bboxes.to(DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpredictions = objectDetector(images)\n",
    "\t\tbboxLoss = bboxLossFunc(predictions[0], bboxes)\n",
    "\t\tclassLoss = classLossFunc(predictions[1], labels)\n",
    "\t\ttotalLoss = (BBOX * bboxLoss) + (LABELS * classLoss)\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\ttotalLoss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += totalLoss\n",
    "\t\ttrainCorrect += (predictions[1].argmax(1) == labels).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\t\t\n",
    "        # switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tobjectDetector.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (images, labels, bboxes) in testLoader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(images, labels, bboxes) = (images.to(DEVICE),\n",
    "\t\t\t\tlabels.to(DEVICE), bboxes.to(DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpredictions = objectDetector(images)\n",
    "\t\t\tbboxLoss = bboxLossFunc(predictions[0], bboxes)\n",
    "\t\t\tclassLoss = classLossFunc(predictions[1], labels)\n",
    "\t\t\ttotalLoss = (BBOX * bboxLoss) + \\\n",
    "\t\t\t\t(LABELS * classLoss)\n",
    "\t\t\ttotalValLoss += totalLoss\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\tvalCorrect += (predictions[1].argmax(1) == labels).type(\n",
    "\t\t\t\ttorch.float).sum().item()\n",
    "\t\t\t\n",
    "            # calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# calculate the training and validation accuracy\n",
    "\ttrainCorrect = trainCorrect / len(trainDS)\n",
    "\tvalCorrect = valCorrect / len(testDS)\n",
    "\t# update our training history\n",
    "\tH[\"total_train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_class_acc\"].append(trainCorrect)\n",
    "\tH[\"total_val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_class_acc\"].append(valCorrect)\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Train : {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n",
    "\tprint(\"Val loss: {:.6f}, Val : {:.4f}\".format(\n",
    "\t\tavgValLoss, valCorrect))\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "torch.save(objectDetector, MODEL_PATH)\n",
    "# serialize the label encoder to disk\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "\n",
    "try:\n",
    "\tf = open(LE_PATH, \"wb\")\n",
    "except:\n",
    "\t# if the file doesn't exist, create the file using OS\n",
    "\tif not os.path.exists(LE_PATH):\n",
    "\t\tos.makedirs(os.path.dirname(LE_PATH))\n",
    "\tf = open(LE_PATH, \"wb\")\n",
    "\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"total_train_loss\"], label=\"total_train_loss\")\n",
    "plt.plot(H[\"total_val_loss\"], label=\"total_val_loss\")\n",
    "plt.plot(H[\"train_class_acc\"], label=\"train_class_acc\")\n",
    "plt.plot(H[\"val_class_acc\"], label=\"val_class_acc\")\n",
    "plt.title(\"Total Training Loss and Classification Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# save the training plot\n",
    "plotPath = os.path.sep.join([PLOTS_PATH, \"training.png\"])\n",
    "try:\n",
    "\tplt.savefig(plotPath)\n",
    "except:\n",
    "\t# if the file doesn't exist, create the file using OS\n",
    "    if not os.path.exists(plotPath):\n",
    "        os.makedirs(os.path.dirname(plotPath))\n",
    "    plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python predict.py --input dataset/images/face/image_0131.jpg\n",
    "# import the necessary packages\n",
    "from torchvision import transforms\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "# construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--input\", required=True,\n",
    "# \thelp=\"path to input image/text file of image paths\")\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the input file type, but assume that we're working with\n",
    "# single input image\n",
    "# filetype = mimetypes.guess_type(args[\"input\"])[0]\n",
    "# imagePaths = [args[\"input\"]]\n",
    "# # if the file type is a text file, then we need to process *multiple*\n",
    "# # images\n",
    "# if \"text/plain\" == filetype:\n",
    "# \t# load the image paths in our testing file\n",
    "# \timagePaths = open(args[\"input\"]).read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our object detector, set it evaluation mode, and label\n",
    "# encoder from disk\n",
    "print(\"[INFO] loading object detector...\")\n",
    "model = torch.load(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "le = pickle.loads(open(LE_PATH, \"rb\").read())\n",
    "# define normalization transforms\n",
    "transforms = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=MEAN, std=STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "with open(TEST_PATHS, 'r') as f:\n",
    "    image_file_paths_list_test = f.readlines()\n",
    "\n",
    "imagePaths = [path.strip() for path in image_file_paths_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the images that we'll be testing using our bounding box\n",
    "# regression model\n",
    "cnt = 0\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, copy it, swap its colors channels, resize it, and\n",
    "\t# bring its channel dimension forward\n",
    "\timage = cv2.imread(imagePath)\n",
    "\torig = image.copy()\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage = image.transpose((2, 0, 1))\n",
    "\t# convert image to PyTorch tensor, normalize it, flash it to the\n",
    "\t# current device, and add a batch dimension\n",
    "\timage = torch.from_numpy(image)\n",
    "\timage = transforms(image).to(DEVICE)\n",
    "\timage = image.unsqueeze(0)\n",
    "\t\n",
    "    \t# predict the bounding box of the object along with the class\n",
    "\t# label\n",
    "\t(boxPreds, labelPreds) = model(image)\n",
    "\t(startX, startY, endX, endY) = boxPreds[0]\n",
    "\t# determine the class label with the largest predicted\n",
    "\t# probability\n",
    "\tlabelPreds = torch.nn.Softmax(dim=-1)(labelPreds)\n",
    "\ti = labelPreds.argmax(dim=-1).cpu()\n",
    "\tlabel = le.inverse_transform(i)[0]\n",
    "\t\n",
    "    \t# resize the original image such that it fits on our screen, and\n",
    "\t# grab its dimensions\n",
    "\torig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "\torig = imutils.resize(orig, width=600)\n",
    "\t(h, w) = orig.shape[:2]\n",
    "\t# scale the predicted bounding box coordinates based on the image\n",
    "\t# dimensions\n",
    "\tstartX = int(startX * w)\n",
    "\tstartY = int(startY * h)\n",
    "\tendX = int(endX * w)\n",
    "\tendY = int(endY * h)\n",
    "\t# draw the predicted bounding box and class label on the image\n",
    "\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\tcv2.putText(orig, classes[label], (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t0.65, (0, 255, 0), 2)\n",
    "\tcv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "\t\t(0, 255, 0), 2)\n",
    "\t# show the output image \n",
    "\t# cv2.imshow(\"Output\", orig)\n",
    "\tplt.imshow(orig)\n",
    "\tplt.savefig(f\"output_{cnt}.jpg\")\n",
    "\t# cv2.waitKey(0)\n",
    "\tcnt+=1\n",
    "\tif cnt==2:\n",
    "\t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
