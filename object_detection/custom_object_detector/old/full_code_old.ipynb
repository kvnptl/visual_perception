{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom object detector\n",
    "\n",
    "Reference: 06 transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import cv2\n",
    "from utils import set_seed, plot_loss_curves, create_confusion_matrix, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Chihuahua', \n",
    "           'Golden_retriever', \n",
    "           'Welsh_springer_spaniel', \n",
    "           'German_shepherd', \n",
    "           'Doberman', \n",
    "           'Boxer', \n",
    "           'Siberian_husky', \n",
    "           'Pug', \n",
    "           'Pomeranian', \n",
    "           'Cardigan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"dataset/standford_dogs_mini_10\")\n",
    "\n",
    "# image files paths and annotations\n",
    "images_path = os.path.join(data_path, \"images\")\n",
    "annotations_path = os.path.join(data_path, \"yolo\", \"annotations\")\n",
    "\n",
    "images_path, annotations_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_copy\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# train_dataloader = dataset.create_dataloader(\n",
    "#     images_path=images_path,\n",
    "#     annotations_path=annotations_path,\n",
    "#     subset=\"train\",\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     pin_memory=PIN_MEMORY,\n",
    "#     transforms=None,\n",
    "#     num_workers=NUM_WORKERS\n",
    "# )\n",
    "\n",
    "# test_dataloader = dataset.create_dataloader(\n",
    "#     images_path=images_path,\n",
    "#     annotations_path=annotations_path,\n",
    "#     subset=\"test\",\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     pin_memory=PIN_MEMORY,\n",
    "#     transforms=None,\n",
    "#     num_workers=NUM_WORKERS\n",
    "# )\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"dataset/standford_dogs_mini_10\")\n",
    "\n",
    "# image files paths and annotations\n",
    "images_path = os.path.join(data_path, \"yolo\", \"image_paths.txt\")\n",
    "annotations_path = os.path.join(data_path, \"yolo\", \"annotations\")\n",
    "\n",
    "images_path, annotations_path\n",
    "\n",
    "train_dataloader, test_dataloader = dataset_copy.create_dataloaders(\n",
    "    images_file_path=images_path,\n",
    "    annotations_path=annotations_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    transforms=None,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"train_dataloader: {train_dataloader}\")\n",
    "print(f\"test_dataloader: {test_dataloader}\")\n",
    "print(f\"class_names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ImageNet mean and standard deviation\n",
    "# MEAN = [0.485, 0.456, 0.406]\n",
    "# STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# # get a single image\n",
    "# img_batch, annotation_batch = next(iter(train_dataloader))\n",
    "# img, annotation = img_batch[0], annotation_batch[0]\n",
    "# print(f\"img.shape: {img.shape}, label: {class_names[int(annotation[0].numpy())]}\")\n",
    "\n",
    "# # visualize\n",
    "# image = img.permute(1, 2, 0).numpy()\n",
    "# image = image * STD + MEAN\n",
    "# image = np.clip(image, 0, 1)\n",
    "# image = (image * 255).astype(np.uint8)\n",
    "\n",
    "# # # show the image\n",
    "# # plt.imshow(image)\n",
    "# # plt.savefig(\"test.png\")\n",
    "\n",
    "# # draw the bounding box\n",
    "# label = annotation[0].numpy()\n",
    "# bbox = annotation[1:].numpy()\n",
    "\n",
    "# # scale the bounding box coordinates\n",
    "# startX = int(bbox[0] * image.shape[1])\n",
    "# startY = int(bbox[1] * image.shape[0])\n",
    "# endX = int(bbox[2] * image.shape[1])\n",
    "# endY = int(bbox[3] * image.shape[0])\n",
    "\n",
    "# # convert PIL image to OpenCV\n",
    "# image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# # draw the bounding box\n",
    "# cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "# cv2.putText(image, str(class_names[int(label)]), (startX, startY + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# # save the image\n",
    "# cv2.imwrite(\"test.png\", image)\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # ImageNet mean and standard deviation\n",
    "# MEAN = [0.485, 0.456, 0.406]\n",
    "# STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# # # Visualize random 9 images in 3x3 grid\n",
    "# for i in range(9):\n",
    "#     plt.subplot(3, 3, i+1)\n",
    "#     idx = random.randint(0, len(train_dataloader.dataset))\n",
    "#     image, annotation = train_dataloader.dataset[idx][0].permute(1, 2, 0).numpy(), train_dataloader.dataset[idx][1]\n",
    "#     # image\n",
    "#     image = image * STD + MEAN\n",
    "#     image = np.clip(image, 0, 1)\n",
    "#     image = (image * 255).astype(np.uint8)\n",
    "#     # plt.imshow(image)\n",
    "#     # annotation\n",
    "#     label = annotation[0].numpy()\n",
    "#     bbox = annotation[1:].numpy()\n",
    "\n",
    "#     # scale the bounding box coordinates\n",
    "#     startX = int(bbox[0] * image.shape[1])\n",
    "#     startY = int(bbox[1] * image.shape[0])\n",
    "#     endX = int(bbox[2] * image.shape[1])\n",
    "#     endY = int(bbox[3] * image.shape[0])\n",
    "\n",
    "#     # convert PIL image to OpenCV\n",
    "#     image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # draw the bounding box\n",
    "#     cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "#     cv2.putText(image, str(class_names[int(label)]), (startX, startY + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(f\"Label: {class_names[int(label)]}\")\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# Create the network\n",
    "weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "basemodel = torchvision.models.resnet50(weights=weights)\n",
    "\n",
    "model = model.ObjectDetector(basemodel, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "x = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))    \n",
    "\n",
    "summary(model=model, \n",
    "        input_size=(1, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "\n",
    "# NOTE: all layers are trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze ResNet50 weights (backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "x = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))    \n",
    "\n",
    "summary(model=model, \n",
    "        input_size=(1, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "\n",
    "# NOTE: all ResNet50 layers are not trainable (False) and only \n",
    "# classification and bbox regression layers are trainable (True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_RATE = 1e-4\n",
    "\n",
    "# Two loss functions\n",
    "classLoss_function = torch.nn.CrossEntropyLoss()\n",
    "bboxLoss_function = torch.nn.MSELoss()\n",
    "loss_fn = (classLoss_function, bboxLoss_function)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                            lr=LR_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine_copy\n",
    "\n",
    "set_seed(seed=8)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "results = engine_copy.train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=EPOCHS,\n",
    "                       device=device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Time elapsed: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "# Save model\n",
    "save_model(model=model, target_dir=\"models\", model_name=f\"model_{EPOCHS}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from utils import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_confusion_matrix\n",
    "\n",
    "create_confusion_matrix(model=model,\n",
    "                        test_loader=test_dataloader,\n",
    "                        class_names=class_names,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "def pred_and_plot_img(model,\n",
    "                      img_path,\n",
    "                      class_names,\n",
    "                      img_size,\n",
    "                      transform,\n",
    "                      device):\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    if transform is not None:\n",
    "        img_transform = transform\n",
    "    else:\n",
    "        img_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        img_tensor = img_transform(img).unsqueeze(0)\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        prediction = model(img_tensor)\n",
    "\n",
    "    classPred = prediction[0]\n",
    "    bboxPred = prediction[1]\n",
    "\n",
    "    (startX, startY, endX, endY) = bboxPred[0]\n",
    "\n",
    "    # determine the class label with the largest predicted\n",
    "\t# probability\n",
    "    classPred = torch.argmax(torch.softmax(classPred[0], dim=1), dim=1).cpu().numpy()\n",
    "    label = class_names[classPred[0]]\n",
    "\n",
    "    # resize the original image such that it fits on our screen, and\n",
    "    # grab its dimensions\n",
    "    orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "    orig = imutils.resize(orig, width=600)\n",
    "    (h, w) = orig.shape[:2]\n",
    "    # scale the predicted bounding box coordinates based on the image\n",
    "    # dimensions\n",
    "    startX = int(startX * w)\n",
    "    startY = int(startY * h)\n",
    "    endX = int(endX * w)\n",
    "    endY = int(endY * h)\n",
    "    # draw the predicted bounding box and class label on the image\n",
    "    y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "    cv2.putText(orig, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.65, (0, 255, 0), 2)\n",
    "    cv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "        (0, 255, 0), 2)\n",
    "    # show the output image \n",
    "    # cv2.imshow(\"Output\", orig)\n",
    "    plt.imshow(orig)\n",
    "    plt.savefig(f\"output_test_pred.jpg\")\n",
    "\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.imshow(img)\n",
    "    # plt.title(f\"Prediction: {class_names[pred_class]} | Prob: {pred_probs.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# from utils import pred_and_plot_img\n",
    "\n",
    "test_dir = \"/home/kpatel2s/kpatel2s/object_detection/custom_object_detector/dataset/standford_dogs_mini_10/images/n02085620-Chihuahua\"\n",
    "\n",
    "num_imgs = 3\n",
    "test_img_path_list = list(Path(test_dir).glob(\"*.jpg\"))\n",
    "test_img_path_sample = random.sample(population=test_img_path_list, k=num_imgs)\n",
    "\n",
    "for test_img in test_img_path_sample:\n",
    "    pred_and_plot_img(model=model,\n",
    "                    img_path=test_img,\n",
    "                    class_names=class_names,\n",
    "                    img_size=(224, 224),\n",
    "                    transform=None,\n",
    "                    device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
