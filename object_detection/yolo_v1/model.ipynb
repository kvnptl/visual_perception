{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv1 architecture\n",
    "\n",
    "This is a YOLO v1 architecture implementation using PyTorch.\n",
    "\n",
    "References:https://youtu.be/n9_XyCGr-MI?si=GNjO1LAW429Ycsdh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    (7, 64, 2, 3), # kernel, channels, stride, padding\n",
    "    (2, 2), # kernel, stride\n",
    "\n",
    "    (3, 192, 1, 1),\n",
    "    (2, 2),\n",
    "\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    (2, 2),\n",
    "    # List: tuple(kernel, channels, stride, padding) and last one is the number of repeats\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4], # 0\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    (2, 2),\n",
    "\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             bias=False,\n",
    "                             **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, in_channels: int=3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self.create_conv_layers(self.architecture)\n",
    "        self.fcs = self.create_fcs(**kwargs)\n",
    "\n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            # Conv block\n",
    "            if type(x) == tuple and len(x) == 4:\n",
    "                layers += [CNNBlock(in_channels=in_channels, \n",
    "                                  out_channels=x[1], \n",
    "                                  kernel_size=x[0], \n",
    "                                  stride=x[2], \n",
    "                                  padding=x[3])]\n",
    "                in_channels = x[1]\n",
    "                \n",
    "            # Maxpool\n",
    "            elif type(x) == tuple and len(x) == 2:\n",
    "                layers += [nn.MaxPool2d(kernel_size=x[0], stride=x[1])]\n",
    "            # Conv repeated block\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [CNNBlock(in_channels=in_channels, \n",
    "                                      out_channels=conv1[1], \n",
    "                                      kernel_size=conv1[0], \n",
    "                                      stride=conv1[2], \n",
    "                                      padding=conv1[3])]\n",
    "                    layers += [CNNBlock(in_channels=conv1[1], \n",
    "                                      out_channels=conv2[1], \n",
    "                                      kernel_size=conv2[0], \n",
    "                                      stride=conv2[2], \n",
    "                                      padding=conv2[3])]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers) # *layers unpacks the list of layers\n",
    "    \n",
    "    def create_fcs(self, split_size=7, num_boxes=2, num_classes=20): # split_size is the grid size\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=1024 * S * S, out_features=4096),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(in_features=4096, out_features=S*S*(C + B*5)), # each cell is 30x1 (20 classes + (1+4) 1st box + (1+4) 2nd box), where (1+4): probability + x1, y1, x2, y2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "YOLOv1                                   [1, 1470]                 --\n",
       "├─Sequential: 1-1                        [1, 1024, 7, 7]           --\n",
       "│    └─CNNBlock: 2-1                     [1, 64, 224, 224]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 224, 224]         9,408\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 224, 224]         128\n",
       "│    │    └─LeakyReLU: 3-3               [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-2                    [1, 64, 112, 112]         --\n",
       "│    └─CNNBlock: 2-3                     [1, 192, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-4                  [1, 192, 112, 112]        110,592\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 192, 112, 112]        384\n",
       "│    │    └─LeakyReLU: 3-6               [1, 192, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                    [1, 192, 56, 56]          --\n",
       "│    └─CNNBlock: 2-5                     [1, 128, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 128, 56, 56]          24,576\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 128, 56, 56]          256\n",
       "│    │    └─LeakyReLU: 3-9               [1, 128, 56, 56]          --\n",
       "│    └─CNNBlock: 2-6                     [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-10                 [1, 256, 56, 56]          294,912\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 256, 56, 56]          512\n",
       "│    │    └─LeakyReLU: 3-12              [1, 256, 56, 56]          --\n",
       "│    └─CNNBlock: 2-7                     [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 256, 56, 56]          65,536\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 256, 56, 56]          512\n",
       "│    │    └─LeakyReLU: 3-15              [1, 256, 56, 56]          --\n",
       "│    └─CNNBlock: 2-8                     [1, 512, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 512, 56, 56]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 512, 56, 56]          1,024\n",
       "│    │    └─LeakyReLU: 3-18              [1, 512, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-9                    [1, 512, 28, 28]          --\n",
       "│    └─CNNBlock: 2-10                    [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-19                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 256, 28, 28]          512\n",
       "│    │    └─LeakyReLU: 3-21              [1, 256, 28, 28]          --\n",
       "│    └─CNNBlock: 2-11                    [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 512, 28, 28]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 512, 28, 28]          1,024\n",
       "│    │    └─LeakyReLU: 3-24              [1, 512, 28, 28]          --\n",
       "│    └─CNNBlock: 2-12                    [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-25                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─BatchNorm2d: 3-26            [1, 256, 28, 28]          512\n",
       "│    │    └─LeakyReLU: 3-27              [1, 256, 28, 28]          --\n",
       "│    └─CNNBlock: 2-13                    [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-28                 [1, 512, 28, 28]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-29            [1, 512, 28, 28]          1,024\n",
       "│    │    └─LeakyReLU: 3-30              [1, 512, 28, 28]          --\n",
       "│    └─CNNBlock: 2-14                    [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-31                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─BatchNorm2d: 3-32            [1, 256, 28, 28]          512\n",
       "│    │    └─LeakyReLU: 3-33              [1, 256, 28, 28]          --\n",
       "│    └─CNNBlock: 2-15                    [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-34                 [1, 512, 28, 28]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-35            [1, 512, 28, 28]          1,024\n",
       "│    │    └─LeakyReLU: 3-36              [1, 512, 28, 28]          --\n",
       "│    └─CNNBlock: 2-16                    [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-37                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─BatchNorm2d: 3-38            [1, 256, 28, 28]          512\n",
       "│    │    └─LeakyReLU: 3-39              [1, 256, 28, 28]          --\n",
       "│    └─CNNBlock: 2-17                    [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-40                 [1, 512, 28, 28]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-41            [1, 512, 28, 28]          1,024\n",
       "│    │    └─LeakyReLU: 3-42              [1, 512, 28, 28]          --\n",
       "│    └─CNNBlock: 2-18                    [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-43                 [1, 512, 28, 28]          262,144\n",
       "│    │    └─BatchNorm2d: 3-44            [1, 512, 28, 28]          1,024\n",
       "│    │    └─LeakyReLU: 3-45              [1, 512, 28, 28]          --\n",
       "│    └─CNNBlock: 2-19                    [1, 1024, 28, 28]         --\n",
       "│    │    └─Conv2d: 3-46                 [1, 1024, 28, 28]         4,718,592\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 1024, 28, 28]         2,048\n",
       "│    │    └─LeakyReLU: 3-48              [1, 1024, 28, 28]         --\n",
       "│    └─MaxPool2d: 2-20                   [1, 1024, 14, 14]         --\n",
       "│    └─CNNBlock: 2-21                    [1, 512, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 14, 14]          524,288\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 14, 14]          1,024\n",
       "│    │    └─LeakyReLU: 3-51              [1, 512, 14, 14]          --\n",
       "│    └─CNNBlock: 2-22                    [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-52                 [1, 1024, 14, 14]         4,718,592\n",
       "│    │    └─BatchNorm2d: 3-53            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─LeakyReLU: 3-54              [1, 1024, 14, 14]         --\n",
       "│    └─CNNBlock: 2-23                    [1, 512, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-55                 [1, 512, 14, 14]          524,288\n",
       "│    │    └─BatchNorm2d: 3-56            [1, 512, 14, 14]          1,024\n",
       "│    │    └─LeakyReLU: 3-57              [1, 512, 14, 14]          --\n",
       "│    └─CNNBlock: 2-24                    [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-58                 [1, 1024, 14, 14]         4,718,592\n",
       "│    │    └─BatchNorm2d: 3-59            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─LeakyReLU: 3-60              [1, 1024, 14, 14]         --\n",
       "│    └─CNNBlock: 2-25                    [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-61                 [1, 1024, 14, 14]         9,437,184\n",
       "│    │    └─BatchNorm2d: 3-62            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─LeakyReLU: 3-63              [1, 1024, 14, 14]         --\n",
       "│    └─CNNBlock: 2-26                    [1, 1024, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-64                 [1, 1024, 7, 7]           9,437,184\n",
       "│    │    └─BatchNorm2d: 3-65            [1, 1024, 7, 7]           2,048\n",
       "│    │    └─LeakyReLU: 3-66              [1, 1024, 7, 7]           --\n",
       "│    └─CNNBlock: 2-27                    [1, 1024, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-67                 [1, 1024, 7, 7]           9,437,184\n",
       "│    │    └─BatchNorm2d: 3-68            [1, 1024, 7, 7]           2,048\n",
       "│    │    └─LeakyReLU: 3-69              [1, 1024, 7, 7]           --\n",
       "│    └─CNNBlock: 2-28                    [1, 1024, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-70                 [1, 1024, 7, 7]           9,437,184\n",
       "│    │    └─BatchNorm2d: 3-71            [1, 1024, 7, 7]           2,048\n",
       "│    │    └─LeakyReLU: 3-72              [1, 1024, 7, 7]           --\n",
       "├─Sequential: 1-2                        [1, 1470]                 --\n",
       "│    └─Flatten: 2-29                     [1, 50176]                --\n",
       "│    └─Linear: 2-30                      [1, 4096]                 205,524,992\n",
       "│    └─Dropout: 2-31                     [1, 4096]                 --\n",
       "│    └─LeakyReLU: 2-32                   [1, 4096]                 --\n",
       "│    └─Linear: 2-33                      [1, 1470]                 6,022,590\n",
       "==========================================================================================\n",
       "Total params: 271,716,734\n",
       "Trainable params: 271,716,734\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 20.29\n",
       "==========================================================================================\n",
       "Input size (MB): 2.41\n",
       "Forward/backward pass size (MB): 220.82\n",
       "Params size (MB): 1086.87\n",
       "Estimated Total Size (MB): 1310.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(YOLOv1(), (1, 3, 448, 448))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/image.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import intersection_over_union\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, split_size=7, num_boxes=2, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.split_size = split_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\") # sum of all elements\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        # We have to reshape as YOLO output is (1, 1470), so convert it to (1, 7, 7, 30)\n",
    "        predictions = prediction.reshape(-1, \n",
    "                                         self.split_size, \n",
    "                                         self.split_size, \n",
    "                                         self.num_classes + self.num_boxes * 5) # -1 means don't touch the first dimension, it could be anything (here batch size)\n",
    "\n",
    "        iou_b1 = intersection_over_union(prediction[..., 21:25], target[..., 21:25]) # [...]: means take all elements, e.g. from (1, 7, 7, 30), take (1, 7, 7) as it is\n",
    "        iou_b2 = intersection_over_union(prediction[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "        iou_maxes, best_box = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3).float() # Identity matrix\n",
    "\n",
    "        # Bounding box regression\n",
    "        box_predictions = exists_box * (\n",
    "            best_box * predictions[..., 26:30] + (1 - best_box) * predictions[..., 21:25]\n",
    "        )\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "\n",
    "        # Object loss\n",
    "        pred_box = (\n",
    "            best_box * predictions[..., 25:26] + (1 - best_box) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        # (N, S, S) -> (N*S*S)\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # No object loss\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1), \n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1), \n",
    "        )\n",
    "\n",
    "        # Class loss\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2), # end_dim=-2: flatten the last dim (N, S, S, 20) -> (N*S*S, 20)\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # Total loss\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + class_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, img_dir, label_dir, split_size=7, num_boxes=2, num_classes=20, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.split_size = split_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if float(x) != int(float(x)) else int(x)\n",
    "                    for x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        boxes = torch.tensor(boxes) # converting to tensor so that we can apply transforms\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        label_matrix = torch.zeros((self.split_size, self.split_size, self.num_classes + self.num_boxes * 5))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            i, j = int(self.split_size * x), int(self.split_size * y)\n",
    "            x_cell, y_cell = self.split_size * x - i, self.split_size * y - j\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.split_size,\n",
    "                height * self.split_size,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                label_matrix[i, j, 20] = 1\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from utils import intersection_over_union, non_max_suppression, mean_average_precision, cellboxes_to_boxes, get_bboxes, plot_image, save_checkpoint, load_checkpoint\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "weight_decay = 0\n",
    "EPOCHS = 100\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "IMG_DIR = \"/srv/disk1/datasets/kpatel2s_datasets/pascal_voc_dataset/images\"\n",
    "LABEL_DIR = \"/srv/disk1/datasets/kpatel2s_datasets/pascal_voc_dataset/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv1(split_size=7, num_boxes=2, num_classes=20)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "loss_fn = YOLOLoss(split_size=7, num_boxes=2, num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "train_dataset = VOCDataset(\n",
    "    csv_file=\"/srv/disk1/datasets/kpatel2s_datasets/pascal_voc_dataset/8examples.csv\",\n",
    "    transform=transform,\n",
    "    img_dir=IMG_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    ")\n",
    "\n",
    "test_dataset = VOCDataset(\n",
    "    csv_file=\"/srv/disk1/datasets/kpatel2s_datasets/pascal_voc_dataset/8examples.csv\",\n",
    "    transform=transform,\n",
    "    img_dir=IMG_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (7) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMean Average Precision: \u001b[39m\u001b[39m{\u001b[39;00mmean_avg_prec\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# if mean_avg_prec > 0.9:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#     checkpoint = {\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#         \"state_dict\": model.state_dict(),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#     import time\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#     time.sleep(10)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m train_fn(train_loader, model, optimizer, loss_fn)\n",
      "\u001b[1;32m/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(out, y)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m mean_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_practice/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, prediction, target):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# We have to reshape as YOLO output is (1, 1470), so convert it to (1, 7, 7, 30)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     predictions \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m                                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m                                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_boxes \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m) \u001b[39m# -1 means don't touch the first dimension, it could be anything (here batch size)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     iou_b1 \u001b[39m=\u001b[39m intersection_over_union(prediction[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m21\u001b[39;49m:\u001b[39m25\u001b[39;49m], target[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m21\u001b[39;49m:\u001b[39m25\u001b[39;49m]) \u001b[39m# [...]: means take all elements, e.g. from (1, 7, 7, 30), take (1, 7, 7) as it is\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     iou_b2 \u001b[39m=\u001b[39m intersection_over_union(prediction[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m26\u001b[39m:\u001b[39m30\u001b[39m], target[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m21\u001b[39m:\u001b[39m25\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22776f726b5f6c61625f636c75737465725f48425253227d/home/kpatel2s/kpatel2s/2d_object_detection/yolo_v1/model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     ious \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([iou_b1\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), iou_b2\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/kpatel2s/2d_object_detection/yolo_v1/utils.py:43\u001b[0m, in \u001b[0;36mintersection_over_union\u001b[0;34m(boxes_preds, boxes_labels, box_format)\u001b[0m\n\u001b[1;32m     40\u001b[0m     box2_x2 \u001b[39m=\u001b[39m boxes_labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m2\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[1;32m     41\u001b[0m     box2_y2 \u001b[39m=\u001b[39m boxes_labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m:\u001b[39m4\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(box1_x1, box2_x1)\n\u001b[1;32m     44\u001b[0m y1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(box1_y1, box2_y1)\n\u001b[1;32m     45\u001b[0m x2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(box1_x2, box2_x2)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (7) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from utils import get_bboxes\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # for x, y in train_loader:\n",
    "    #        x = x.to(device)\n",
    "    #        for idx in range(8):\n",
    "    #            bboxes = cellboxes_to_boxes(model(x))\n",
    "    #            bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "    #            plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "    )\n",
    "\n",
    "    print(f\"Mean Average Precision: {mean_avg_prec}\")\n",
    "\n",
    "    # if mean_avg_prec > 0.9:\n",
    "    #     checkpoint = {\n",
    "    #         \"state_dict\": model.state_dict(),\n",
    "    #         \"optimizer\": optimizer.state_dict(),\n",
    "    #     }\n",
    "    #     save_checkpoint(checkpoint, \"overfit.pth.tar\")\n",
    "    #     import time\n",
    "    #     time.sleep(10)\n",
    "\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
